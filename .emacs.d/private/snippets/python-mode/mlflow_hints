# -*- mode: snippet -*-
#name : mlflow_hints
#key : mlflow_hints
#contributor : Ubuntu
# --

# see: /workspaces/ilt-csci-e-104-advanced-deep-learning/tracking_quickstart.ipynb
# ## MLflow 5 minute Tracking Quickstart
# This notebook demonstrates using a local MLflow Tracking Server to log, register, and then load a model as a generic Python Function (pyfunc) to perform inference on a Pandas DataFrame.
# Throughout this notebook, we'll be using the MLflow fluent API to perform all interactions with the MLflow Tracking Server.
import mlflow
from mlflow.models import infer_signature
# ### Set the MLflow Tracking URI 
# Depending on where you are running this notebook, your configuration may vary for how you initialize the interface with the MLflow Tracking Server. 
# Please see [the guide to running notebooks here](https://www.mlflow.org/docs/latest/getting-started/running-notebooks/index.html) for more information on setting the tracking server uri and configuring access to either managed or self-managed MLflow tracking servers.
# mlflow.set_tracking_uri(uri="http://127.0.0.1:5000")
mlflow.set_tracking_uri(uri="http://127.0.0.1:3000")
mlflow.set_experiment("/check_localhost")
with mlflow.start_run():
    mlflow.log_metric("foo", 1)
    mlflow.log_metric("bar", 2)
# ### View Experiment on Your MLflow Server
# Note that the only MLflow-related activities in this portion are around the fact that we're using a `param` dictionary to supply our model's hyperparameters; this is to make logging these settings easier when we're ready to log our model and its associated metadata.
# ## Define an MLflow Experiment
mlflow.set_experiment("MLflow Iris Experiment")
# ## Log the model, hyperparameters, and loss metrics to MLflow.
# In order to record our model and the hyperparameters that were used when fitting the model, as well as the metrics associated with validating the fit model upon holdout data, we initiate a run context, as shown below. Within the scope of that context, any fluent API that we call (such as `mlflow.log_params()` or `mlflow.sklearn.log_model()`) will be associated and logged together to the same run. 
# Start an MLflow run
with mlflow.start_run():
    mlflow.log_params(params)
    mlflow.log_metric("accuracy", accuracy)
    mlflow.set_tag("Training Info", "Basic LR model for iris data")
    model_info = mlflow.sklearn.log_model(
# If you scroll down the page of MLflow server, you will see that tracking server kept information on the model itself, including the Python code, python environment, conda environment, etc. 
# Although we can load our model back as a native scikit-learn format with `mlflow.sklearn.load_model()`, below we are loading the model as a generic Python Function, which is how this model would be loaded for online model serving. We can still use the `pyfunc` representation for batch use cases, though, as is shown below.
loaded_model = mlflow.pyfunc.load_model(model_info.model_uri)
 
